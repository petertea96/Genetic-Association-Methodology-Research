names(cran)
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran,
| r_version == "3.1.1", country == "US")
filter(cran,r_version == "3.1.1", country == "US")
?Comparison
filter(cran,r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size> 100500, r_os =="linus-gnu")
filter(cran, size> 100500, r_os =="linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(r_version))
select(cran, size:ip_id)
cran2=select(cran, size:ip_id)
cran2<-select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran = tbl_df(mydf)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
?n
?n_distinct
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count>679 )
top_counts
View(top_counts)
top_counts <- arrance(pack_sum, desc(count))
top_counts <- arrange(pack_sum, desc(count))
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique>465)
View(top_unique)
top_unique_sorted = arrange(top_unique, desc(unique()))
top_unique_sorted = arrange(top_unique, desc(unique)
)
top_unique_sorted <- arrange(top_unique, desc(unique))
View()
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
source('C:/Users/Peter/AppData/Local/Temp/RtmpSe8IlZ/chain1.R')
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
install.packages("APE")
paste("Rep.",1:1000, sep=" ")
rep(paste("Rep.",1:1000, sep=" "),3)
rep(paste("Rep.",1:10, sep=" "),3)
sort(rep(paste("Rep.",1:10, sep=" "),3))
?rep
rep(1:10, times=3)
sort(rep(1:10, times=3))
rep(1:1000, times=3)  %>%
sort
library(dplyr)
rep(1:1000, times=3)  %>%
sort
load("C:/Users/Peter/Documents/Uottawa/Summer Research 2018/Code to run on cluster/.RData")
View(Pheno1)
install.packages("gridExtra")
grid.table(Pheno1)
library(gridExtra)
grid.table(Pheno1)
grid.table(Pheno2)
grid.table(Pheno1)
png(filename = "pheno1.png", width=480,height=480,bg = "white")
grid.table(Pheno1)
dev.off()
Pheno1
png(filename = "pheno1.png", height = 50*nrow(Pheno1), width = 200*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 100*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = nrow(Pheno1), width = ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 50*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 75*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
library(swirl)
swirl()
str(mpg)
qplot(displ, hwy, data = mpg)
qplot(displ, hwy, data = mpg, color=drv)
qplot(displ, hwy, data = mpg, color=drv, geom=("point", "smooth"))
qplot(displ, hwy, data = mpg, color=drv, geom=c("point", "smooth"))
qplot(y=hwy, data=mpg, color=drv)
myhigh
qplot(drv, hwy, data=mpg, geom="boxplot")
qplot(drv, hwy, data=mpg, geom="boxplot", color=manufacturer)
qplot(hwy, data=mpg, fill=drv)
qplot(displ, hwy, data=mpg, facets=.~drv)
qplot(hwy, data=mpg, facets=.~drv, binwidth=2)
qplot(hwy, data=mpg, facets=drv~., binwidth=2)
qplot(displ, hwy, data=mpg, geom=c("point, smooth"), facets= .~drv)
qplot(displ, hwy, data=mpg, geom=c("point", "smooth"), facets= .~drv)
g=ggplot(mpg aes=c(displ,hwy))
g<-ggplot(mpg, aes=c(displ,hwy))
g <- ggplot(mpg, aes(displ,hwy))
summary(g)
g + geom_point()
g + geom_point() + geom_smooth()
g + geom_point() + geom_smooth(method = "lm")
g + geom_point() + geom_smooth(method = "lm") + facet_grid(.~drv)
g + geom_point() + geom_smooth(method = "lm") + facet_grid(.~drv) + ggtitle("Swirl Rules!")
g + geom_point(color="pink")
g + geom_point(color="pink", size=4)
g + geom_point(color="pink", size=4, alpha=1/2)
g + geom_point(size=4, alpha=1/2)
g + geom_point(size=4, alpha=1/2, aes(color=drv))
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement")
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement", y="Hwy Mileage")
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement", y="Hwy Mileage")
g + geom_point(aes(color = drv)) + labs(title="Swirl Rules!") + labs(x="Displacement",y="Hwy Mileage")
g + geom_point(aes(color=drv), size=2, alpha=1/2) + geom_smooth(size=4, linetype=3, method="lm", se=FALSE)
g + geom_point(aes(color=drv)) + theme_bw(base_family = "Times")
qplot(myx, myy, type="l")
plot(myx, myy, type="l", ylim=c(-3,3))
g <- ggplot(testdat, aes(x=myx, y=myy))
g + geom_line()
g + geom_line() + ylim(-3,3)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
g = ggplot()
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g + geom_point()
g + geom_point() + facet_grid(drv~cyl, margins = TRUE)
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm", se=FALSE, color="black")
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm",size=2, se=FALSE, color="black")
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm",size=2, se=FALSE, color="black") + labs(x="Displacement", y="Highway Mileage", title="Swirl Rules!")
str(diamonds)
qplot(price, data=diamonds)
range(diamonds$price)
qplot(price, data=diamonds, binwidth=18497/30)
brk
counts
qplot(price, data=diamonds, binwidth=18497/30, fill=cut())
qplot(price, data=diamonds, binwidth=18497/30, fill=cut
)
qplot(price, data=diamonds, geom="density")
qplot(price, data=diamonds, geom="density", color=cut)
qplot(carat, price, data=diamonds)
qplot(carat, price, data=diamonds, shape=cut)
qplot(carat, price, data=diamonds, color=cut)
qplot(carat, price, data=diamonds, color=cut, geom_smooth(method="lm"))
qplot(carat, price, data=diamonds, color=cut)
qplot(carat,price,data=diamonds, color=cut) + geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut) + geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut, facets=.~cut) + geom_smooth(method="lm")
g=ggplot(diamonds, aes(depth, price))
g<-ggplot(diamonds, aes(depth, price))
summary(g)
g + geom_point()
g + geom_point(alpha=1/3)
cutpoints <- quantile(diamonds$carat, seq(0,1,length=4), na.rm=TRUE)
cutpoints
diamonds$car2 <- cut(diamonds$carat, cutpoints)
diamonds$car2
g <- ggplot(diamonds,aes(depth,price))
g + geom_point(alpha=1/3) + facet_grid(cut~car2)
diamonds[myd,]
g + geom_point(alpha=1/3) + facet_grid(cut~car2) + geom_smooth(method="lm", size=3, color="pink")
g + geom_point(alpha=1/3) + facet_grid(cut~car2) + geom_smooth(method="lm", size=3, color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
?print.trellis
?trellis.par.set
?write.table
r = rnorm(10)
write.table(r, file="GG.txt")
install.packages("mdmr")
install.packages("MDMR")
library(MDMR)
?seq
Beta = seq(from=0.25, to = 1, by = 0.05)
Beta
knitr::opts_chunk$set(echo = TRUE)
data =c(160, 175, 180, 185, 185, 185, 190, 190, 195, 195, 195, 200, 200,
200, 200, 205, 205, 210, 210, 218, 219, 220, 222, 225, 225, 232)
data>215
p_hat = sum(data>215) / length(data)
p_hat
?rchisq
qchisq(p=0.025, df=2*n, lower.tail = FALSE)
data.3 = c(1, 4, 5, 21, 22, 28, 40, 42, 51, 53,
58, 67, 95, 124, 124, 160, 202, 260, 303, 363)
n = length(data.3)
qchisq(p=0.025, df=2*n, lower.tail = FALSE)
qchisq(p=0.975, df=2*n, lower.tail = FALSE)
qchisq(p=0.975, df=2*n)
qchisq(p=0.025, df=2*n)
data.3 = c(1, 4, 5, 21, 22, 28, 40, 42, 51, 53,
58, 67, 95, 124, 124, 160, 202, 260, 303, 363)
n = length(data.3)
my.sum = sum(data.3)
lower = 2*my.sum/qchisq(p=0.975, df=2*n)
upper =  2*my.sum/qchisq(p=0.025, df=2*n)
print(c(lower,upper))
qnorm(p=0.025)
?qnorm
qnorm(p=0.025, lower.tail = FALSE)
qnorm(p=0.05, lower.tail = FALSE)
(3*qnorm(p=0.05, lower.tail = FALSE))^2
?tnorm
qt(p=0.025)
qt(p=0.025, df=7)
qt(p=0.975, df=7)
qt(p=0.9, df=7)
qt(p=0.1, df=7)
qt(p=0.9, df=7)*sqrt(9)/sqrt(8)
1-.954
1-.954/2
(1-.954)/2
1-0.023
qnorm(p=0.977)
qnorm(p=0.9)
qnorm(p=0.95)
?qnorm
qnorm(p=0.1)
qnorm(p=0.95)
qnorm(p=0.1, lower.tail = FALSE)
qnorm(p=0.95, lower.tail = FALSE)
getwd()
?qf
qf(p=0.05, 1,34)
qf(95, 1,34)
qf(.95, 1,34)
qf(.96, 1,34)
qf(.94, 1,34)
library(caret)
library(kernlab)
data(spam)
folds = createFolds(y = spam$type, k = 10,
list = TRUE, returnTrain = TRUE)
sapply(folds, length)
nrow(spam)
?createFolds
folds
fold[1]
folds[1]
length(fold[1])
length(folds[1])
length(folds[[1])
length(folds[[1]])
nrow(spam)
4601/10
4141 + 460
folds = createFolds(y = spam$type, k = 10,
list = TRUE, returnTrain = FALSE)
sapply(folds, length)
my_model = train(type~., data=training_data, method="glm")
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
training_data = spam[inTrain_indices,]
testing_data = spam[-inTrain_indices,]
my_model = train(type~., data=training_data, method="glm")
install.packages("AppliedPredictiveModeling")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
names(adData)
colnames(adData)
?grep1
?grepl
grep1(pattern = "IL", x = "IL")
grepl(pattern="IL", x="IL")
grepl(pattern="IL", x=c("IL", "LKIL")
)
which(pattern = "IL", test = colnames(adData))
which(grep1(pattern = "IL", test = colnames(adData)))
which(grep1(pattern = "IL", test = colnames(adData)))
grep1
grep1(pattern = "IL", test = colnames(adData))
grepl(pattern = "IL", test = colnames(adData))
colnames(adData)
mynames = colnames(adData)
grepl(pattern = "IL", test = mynames)
grepl(pattern = "IL", test = "IL")
grepl(pattern = "IL", text = mynames)
grepl(pattern = "IL", x = mynames)
which(grepl(pattern = "IL", x = colnames(adData)))
colnames(adData)[58]
colnames(adData)[59]
colnames(adData)[60]
colnames(adData)[61]
colnames(adData)[111]
colnames(adData)[69]
library(mtcars)
install.packages("mtcars")
data(mtcars)
data(mtcars)
ls
ls()
View(mtcars)
library(kernlab)
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
?prcomp
mtcars.pca = prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
summary(mtcars.pca)
install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(grepl(pattern = "IL", x = colnames(adData)))
?preProcess
pca = preProcess(training_data[,58:69], method = "pca")
pca = preProcess(training[,58:69], method = "pca")
pca
summary(pca)
trainPC = predict(pca, training[,58:69])
summary(trainPC)
a = prcomp(training[,58:69], center = TRUE,scale. = TRUE)
summary(a)
trainPC
summary(trainPC)
pca$rotation
a$rotation
my_model = train(diagnosis ~ training[,58:69], data=training, method="glm")
?train
training = training[, 58:69]
my_model = train(diagnosis ~., data=training, method="glm")
library(ISLR); library(ggplot2); library(caret)
data(Wage)
inTrain = createDataPartition(y = Wage$wage,
p=0.7, list=FALSE)
training_data = Wage[inTrain, ]; testing_data = Wage[-inTrain, ]
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
data(spam)
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
training_data = spam[inTrain_indices,]; testing_data = spam[-inTrain_indices,]
hist(training_data$capitalAve)
mean(training_data$capitalAve); sd(training_data$capitalAve)
standardised_capitalAve = (training_data$capitalAve - mean(training_data$capitalAve) ) / sd(training_data$capitalAve)
mean(standardised_capitalAve); sd(standardised_capitalAve)
my_standard = preProcess(training_data, method = c("center", "scale"))
head(my_standard)
standardised_capitalAve
?predict
mean(my_model); sd(my_model)
my_model = predict(my_standard, training_data)$capitalAve
mean(my_model); sd(my_model)
?predict
my_model = predict(object = my_standard, newdata=training_data)$capitalAve
mean(my_model); sd(my_model)
?preProcess
?knn
length("0000000010000011101111111000000001001010101100110000010001100000000111100101101111001010000001001000011")
length(0000000010000011101111111000000001001010101100110000010001100000000111100101101111001010000001001000011length())
P1 = rnorm(3)
P1
phenotype1_model = lm(P1~1)
residuals_P1 = resid(phenotype1_model)
residuals_P1
Z_ij = residuals_P1%*%t(residuals_P1)
Z_ij
N = sum(upper.tri(Z_ij))
N
upper.tri(Z_ij)
z1 <-c(rep (0,N))
n=3
count = 1
for (i in 1:(n -1)) {
for (j in (i +1): n) {
z1[ count ]= Z_ij[i,j]
count = count + 1
}
}
z1
Z_ij
getwd()
list.files()
file_number = 100
directory_name = paste("Run_", file_number)
setwd(directory_name)
getwd()
directory_name
directory_name = paste("Run_", file_number, sep="")
setwd(directory_name)
getwd()
haplodat = read.delim("crohn5q31_haplo.dat", colClasses = "character")
View(haplodat)
nrow(haplodat)
?read.delim
directory_name = paste("Run_", file_number, sep="", header=FALSE)
directory_name = paste("Run_", file_number, sep="")
haplodat = read.delim("crohn5q31_haplo.dat", colClasses = "character", header=FALSE)
nrow(haplodat)
dim(haplodat)
FirstLine=unlist(strsplit(haplodat[1,1],split=""))
segsites=length(FirstLine)
FirstLine
View(haplodat)
segsites
newhaplodat=matrix(as.numeric(unlist(strsplit(haplodat[,1],split=""))),
ncol=segsites,byrow=T)
dim(newhaplodat)
View(hewhaplodat)
View(newhaplodat)
f1=colSums(newhaplodat)/nrow(newhaplodat)
tochange=which(f1>0.5)
to change
tochange
if (length(tochange) !=0){
for (i in 1:length(tochange)){
index=tochange[i]
newhaplodat[,index]=1-newhaplodat[,index]
}
}
View(newhaplodat)
if (nrow(newhaplodat) %% 2 == 1){
status_phenotype=rep(c(1,0),ceiling( nrow(newhaplodat)/2) )[-length(status_phenotype)]
}
if (nrow(newhaplodat) %% 2 == 1){
status_phenotype=rep(c(1,0),ceiling( nrow(newhaplodat)/2) )[-length(status_phenotype)]
}else {
status_phenotype=rep(c(1,0),nrow(newhaplodat)/2)
}
status_phenotype
length(status_phenotype)
my_indices = 1:300
my_indices
my_indices = seq(1,300, by=3)
my_indices
SKAT_indices = seq(1,300, by=3)
GTSM_indices = seq(2,300, by = 3)
GTSM_indices
len(GTSM_indices)
length(GTSM_indices)
MDMR_indices=seq(3, 300, by=3)
setwd("C:/Users/Peter/Documents/Uottawa/2018 - 2019 Honour's project/Cluster files/Final codes/Genetic-Association-Methodology-Research/Crohns Disease/Tree Kernel Analysis")
GTSR_data = read.table("GTSR_mean_results.txt" )
MDMR_data = read.table("MDMR_mean_results.txt")
SKAT_data = read.table("SKAT_mean_results.txt")
my_column_names= c("Tree1", "Tree2", "Tree3", "Tree4", "Tree5" )
colnames(GTSR_data) = my_column_names
colnames(MDMR_data) = my_column_names
colnames(SKAT_data) = my_column_names
library(reshape)
GTSR_reshaped = melt(na.omit(GTSR_data))
MDMR_reshaped = melt(na.omit(MDMR_data))
SKAT_reshaped = melt(na.omit(SKAT_data))
library(ggplot2)
ggplot(GTSR_reshaped, aes(x=variable, y = value, fill=variable)) +
geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4) +
ggtitle("SimReg - Tree Kernel association statistic distributions") +
xlab("Tree kernel") + ylab("Statistic value") +
theme_classic()
ggplot(MDMR_reshaped, aes(x=variable, y = value, fill=variable)) +
geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4) +
ggtitle("MDMR - Tree Kernel association statistic distributions") +
xlab("Tree kernel") + ylab("Statistic value") +
theme_classic()
ggplot(SKAT_reshaped, aes(x=variable, y = value, fill=variable)) +
geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4) +
ggtitle("SKAT - Tree Kernel association statistic distributions") +
xlab("Tree kernel") + ylab("Statistic value") +
theme_classic()
