paste("Rep.",1:1000, sep=" ")
rep(paste("Rep.",1:1000, sep=" "),3)
rep(paste("Rep.",1:10, sep=" "),3)
sort(rep(paste("Rep.",1:10, sep=" "),3))
?rep
rep(1:10, times=3)
sort(rep(1:10, times=3))
rep(1:1000, times=3)  %>%
sort
library(dplyr)
rep(1:1000, times=3)  %>%
sort
load("C:/Users/Peter/Documents/Uottawa/Summer Research 2018/Code to run on cluster/.RData")
View(Pheno1)
install.packages("gridExtra")
grid.table(Pheno1)
library(gridExtra)
grid.table(Pheno1)
grid.table(Pheno2)
grid.table(Pheno1)
png(filename = "pheno1.png", width=480,height=480,bg = "white")
grid.table(Pheno1)
dev.off()
Pheno1
png(filename = "pheno1.png", height = 50*nrow(Pheno1), width = 200*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 100*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = nrow(Pheno1), width = ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 50*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
png(filename = "pheno1.png", height = 25*nrow(Pheno1), width = 75*ncol(Pheno1),bg = "white")
grid.table(Pheno1)
dev.off()
library(swirl)
swirl()
str(mpg)
qplot(displ, hwy, data = mpg)
qplot(displ, hwy, data = mpg, color=drv)
qplot(displ, hwy, data = mpg, color=drv, geom=("point", "smooth"))
qplot(displ, hwy, data = mpg, color=drv, geom=c("point", "smooth"))
qplot(y=hwy, data=mpg, color=drv)
myhigh
qplot(drv, hwy, data=mpg, geom="boxplot")
qplot(drv, hwy, data=mpg, geom="boxplot", color=manufacturer)
qplot(hwy, data=mpg, fill=drv)
qplot(displ, hwy, data=mpg, facets=.~drv)
qplot(hwy, data=mpg, facets=.~drv, binwidth=2)
qplot(hwy, data=mpg, facets=drv~., binwidth=2)
qplot(displ, hwy, data=mpg, geom=c("point, smooth"), facets= .~drv)
qplot(displ, hwy, data=mpg, geom=c("point", "smooth"), facets= .~drv)
g=ggplot(mpg aes=c(displ,hwy))
g<-ggplot(mpg, aes=c(displ,hwy))
g <- ggplot(mpg, aes(displ,hwy))
summary(g)
g + geom_point()
g + geom_point() + geom_smooth()
g + geom_point() + geom_smooth(method = "lm")
g + geom_point() + geom_smooth(method = "lm") + facet_grid(.~drv)
g + geom_point() + geom_smooth(method = "lm") + facet_grid(.~drv) + ggtitle("Swirl Rules!")
g + geom_point(color="pink")
g + geom_point(color="pink", size=4)
g + geom_point(color="pink", size=4, alpha=1/2)
g + geom_point(size=4, alpha=1/2)
g + geom_point(size=4, alpha=1/2, aes(color=drv))
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement")
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement", y="Hwy Mileage")
g + geom_point(aes(color=drv)) + labs(title="Swirl Rules!", x="Displacement", y="Hwy Mileage")
g + geom_point(aes(color = drv)) + labs(title="Swirl Rules!") + labs(x="Displacement",y="Hwy Mileage")
g + geom_point(aes(color=drv), size=2, alpha=1/2) + geom_smooth(size=4, linetype=3, method="lm", se=FALSE)
g + geom_point(aes(color=drv)) + theme_bw(base_family = "Times")
qplot(myx, myy, type="l")
plot(myx, myy, type="l", ylim=c(-3,3))
g <- ggplot(testdat, aes(x=myx, y=myy))
g + geom_line()
g + geom_line() + ylim(-3,3)
g + geom_line() + coord_cartesian(ylim=c(-3,3))
g = ggplot()
g <- ggplot(mpg,aes(x=displ,y=hwy,color=factor(year)))
g + geom_point()
g + geom_point() + facet_grid(drv~cyl, margins = TRUE)
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm", se=FALSE, color="black")
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm",size=2, se=FALSE, color="black")
g + geom_point() + facet_grid(drv~cyl, margins = TRUE) + geom_smooth(method="lm",size=2, se=FALSE, color="black") + labs(x="Displacement", y="Highway Mileage", title="Swirl Rules!")
str(diamonds)
qplot(price, data=diamonds)
range(diamonds$price)
qplot(price, data=diamonds, binwidth=18497/30)
brk
counts
qplot(price, data=diamonds, binwidth=18497/30, fill=cut())
qplot(price, data=diamonds, binwidth=18497/30, fill=cut
)
qplot(price, data=diamonds, geom="density")
qplot(price, data=diamonds, geom="density", color=cut)
qplot(carat, price, data=diamonds)
qplot(carat, price, data=diamonds, shape=cut)
qplot(carat, price, data=diamonds, color=cut)
qplot(carat, price, data=diamonds, color=cut, geom_smooth(method="lm"))
qplot(carat, price, data=diamonds, color=cut)
qplot(carat,price,data=diamonds, color=cut) + geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut) + geom_smooth(method="lm")
qplot(carat,price,data=diamonds, color=cut, facets=.~cut) + geom_smooth(method="lm")
g=ggplot(diamonds, aes(depth, price))
g<-ggplot(diamonds, aes(depth, price))
summary(g)
g + geom_point()
g + geom_point(alpha=1/3)
cutpoints <- quantile(diamonds$carat, seq(0,1,length=4), na.rm=TRUE)
cutpoints
diamonds$car2 <- cut(diamonds$carat, cutpoints)
diamonds$car2
g <- ggplot(diamonds,aes(depth,price))
g + geom_point(alpha=1/3) + facet_grid(cut~car2)
diamonds[myd,]
g + geom_point(alpha=1/3) + facet_grid(cut~car2) + geom_smooth(method="lm", size=3, color="pink")
g + geom_point(alpha=1/3) + facet_grid(cut~car2) + geom_smooth(method="lm", size=3, color="pink")
ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
?print.trellis
?trellis.par.set
?write.table
r = rnorm(10)
write.table(r, file="GG.txt")
install.packages("mdmr")
install.packages("MDMR")
library(MDMR)
?seq
Beta = seq(from=0.25, to = 1, by = 0.05)
Beta
knitr::opts_chunk$set(echo = TRUE)
data =c(160, 175, 180, 185, 185, 185, 190, 190, 195, 195, 195, 200, 200,
200, 200, 205, 205, 210, 210, 218, 219, 220, 222, 225, 225, 232)
data>215
p_hat = sum(data>215) / length(data)
p_hat
?rchisq
qchisq(p=0.025, df=2*n, lower.tail = FALSE)
data.3 = c(1, 4, 5, 21, 22, 28, 40, 42, 51, 53,
58, 67, 95, 124, 124, 160, 202, 260, 303, 363)
n = length(data.3)
qchisq(p=0.025, df=2*n, lower.tail = FALSE)
qchisq(p=0.975, df=2*n, lower.tail = FALSE)
qchisq(p=0.975, df=2*n)
qchisq(p=0.025, df=2*n)
data.3 = c(1, 4, 5, 21, 22, 28, 40, 42, 51, 53,
58, 67, 95, 124, 124, 160, 202, 260, 303, 363)
n = length(data.3)
my.sum = sum(data.3)
lower = 2*my.sum/qchisq(p=0.975, df=2*n)
upper =  2*my.sum/qchisq(p=0.025, df=2*n)
print(c(lower,upper))
qnorm(p=0.025)
?qnorm
qnorm(p=0.025, lower.tail = FALSE)
qnorm(p=0.05, lower.tail = FALSE)
(3*qnorm(p=0.05, lower.tail = FALSE))^2
?tnorm
qt(p=0.025)
qt(p=0.025, df=7)
qt(p=0.975, df=7)
qt(p=0.9, df=7)
qt(p=0.1, df=7)
qt(p=0.9, df=7)*sqrt(9)/sqrt(8)
1-.954
1-.954/2
(1-.954)/2
1-0.023
qnorm(p=0.977)
qnorm(p=0.9)
qnorm(p=0.95)
?qnorm
qnorm(p=0.1)
qnorm(p=0.95)
qnorm(p=0.1, lower.tail = FALSE)
qnorm(p=0.95, lower.tail = FALSE)
getwd()
?qf
qf(p=0.05, 1,34)
qf(95, 1,34)
qf(.95, 1,34)
qf(.96, 1,34)
qf(.94, 1,34)
library(caret)
library(kernlab)
data(spam)
folds = createFolds(y = spam$type, k = 10,
list = TRUE, returnTrain = TRUE)
sapply(folds, length)
nrow(spam)
?createFolds
folds
fold[1]
folds[1]
length(fold[1])
length(folds[1])
length(folds[[1])
length(folds[[1]])
nrow(spam)
4601/10
4141 + 460
folds = createFolds(y = spam$type, k = 10,
list = TRUE, returnTrain = FALSE)
sapply(folds, length)
my_model = train(type~., data=training_data, method="glm")
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
training_data = spam[inTrain_indices,]
testing_data = spam[-inTrain_indices,]
my_model = train(type~., data=training_data, method="glm")
install.packages("AppliedPredictiveModeling")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
head(training)
names(adData)
colnames(adData)
?grep1
?grepl
grep1(pattern = "IL", x = "IL")
grepl(pattern="IL", x="IL")
grepl(pattern="IL", x=c("IL", "LKIL")
)
which(pattern = "IL", test = colnames(adData))
which(grep1(pattern = "IL", test = colnames(adData)))
which(grep1(pattern = "IL", test = colnames(adData)))
grep1
grep1(pattern = "IL", test = colnames(adData))
grepl(pattern = "IL", test = colnames(adData))
colnames(adData)
mynames = colnames(adData)
grepl(pattern = "IL", test = mynames)
grepl(pattern = "IL", test = "IL")
grepl(pattern = "IL", text = mynames)
grepl(pattern = "IL", x = mynames)
which(grepl(pattern = "IL", x = colnames(adData)))
colnames(adData)[58]
colnames(adData)[59]
colnames(adData)[60]
colnames(adData)[61]
colnames(adData)[111]
colnames(adData)[69]
library(mtcars)
install.packages("mtcars")
data(mtcars)
data(mtcars)
ls
ls()
View(mtcars)
library(kernlab)
mtcars.pca <- prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
?prcomp
mtcars.pca = prcomp(mtcars[,c(1:7,10,11)], center = TRUE,scale. = TRUE)
summary(mtcars.pca)
install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(grepl(pattern = "IL", x = colnames(adData)))
?preProcess
pca = preProcess(training_data[,58:69], method = "pca")
pca = preProcess(training[,58:69], method = "pca")
pca
summary(pca)
trainPC = predict(pca, training[,58:69])
summary(trainPC)
a = prcomp(training[,58:69], center = TRUE,scale. = TRUE)
summary(a)
trainPC
summary(trainPC)
pca$rotation
a$rotation
my_model = train(diagnosis ~ training[,58:69], data=training, method="glm")
?train
training = training[, 58:69]
my_model = train(diagnosis ~., data=training, method="glm")
library(ISLR); library(ggplot2); library(caret)
data(Wage)
inTrain = createDataPartition(y = Wage$wage,
p=0.7, list=FALSE)
training_data = Wage[inTrain, ]; testing_data = Wage[-inTrain, ]
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
data(spam)
inTrain_indices = createDataPartition(y = spam$type,
p = 0.75, list=FALSE)
training_data = spam[inTrain_indices,]; testing_data = spam[-inTrain_indices,]
hist(training_data$capitalAve)
mean(training_data$capitalAve); sd(training_data$capitalAve)
standardised_capitalAve = (training_data$capitalAve - mean(training_data$capitalAve) ) / sd(training_data$capitalAve)
mean(standardised_capitalAve); sd(standardised_capitalAve)
my_standard = preProcess(training_data, method = c("center", "scale"))
head(my_standard)
standardised_capitalAve
?predict
mean(my_model); sd(my_model)
my_model = predict(my_standard, training_data)$capitalAve
mean(my_model); sd(my_model)
?predict
my_model = predict(object = my_standard, newdata=training_data)$capitalAve
mean(my_model); sd(my_model)
?preProcess
?knn
length("0000000010000011101111111000000001001010101100110000010001100000000111100101101111001010000001001000011")
length(0000000010000011101111111000000001001010101100110000010001100000000111100101101111001010000001001000011length())
P1 = rnorm(3)
P1
phenotype1_model = lm(P1~1)
residuals_P1 = resid(phenotype1_model)
residuals_P1
Z_ij = residuals_P1%*%t(residuals_P1)
Z_ij
N = sum(upper.tri(Z_ij))
N
upper.tri(Z_ij)
z1 <-c(rep (0,N))
n=3
count = 1
for (i in 1:(n -1)) {
for (j in (i +1): n) {
z1[ count ]= Z_ij[i,j]
count = count + 1
}
}
z1
Z_ij
source('~/Uottawa/2018 - 2019 Honour\'s project/Cluster files/Final codes/Crohns Disease/Data/Crohn_order.R')
source('~/Uottawa/2018 - 2019 Honour\'s project/Cluster files/Final codes/Crohns Disease/Data/Crohn_Treesimilarity.R')
library(ape)
library(SKAT)
transmit=cbind(1:(4*129), rep(c(1,0),2*129))
status = transmit[,2]
setwd("C:/Users/Peter/Documents/Uottawa/2018 - 2019 Honour's project/Cluster files/Final codes/Crohns Disease/Data")
trees = read.tree("Merge8Million_trees_1.out")
assn.mat.sub=matrix(ncol=4,nrow=length(trees))
K=floor(129*4/25)
j=1
mytree = trees[[j]]
get.tree.kernels = function(mytree){
# Various (tree) similarity matrices
sim1=treeSimilarity(mytree) # TMRCA - time to present of pairs mrca
sim2=treeSimilarity(mytree,propn=TRUE) # (TMRCA - time to present of pairs mrca)/TMRCA
sim3=treeSimilarity(mytree, method="ranks") # Rank of coalescent event for each pairs mrca (oldest ranked 1)
sim4=treeSimilarity(mytree, method="sdscaled") # Like 1, but intercoalescence times divided by mean/sd
sim5=treeSimilarity(mytree, method="sdscaled",propn=T) # Like 2, but intercoalescence times divided by mean/sd
#Sorted issue:
a1=order(sim1[[1]], sim1[[2]])
a2=order(sim1[[1]], sim1[[2]])
a3=order(sim1[[1]], sim1[[2]])
a4=order(sim1[[1]], sim1[[2]])
a5=order(sim1[[1]], sim1[[2]])
#Kernel Matrices
kernel.list = list(tree1, tree2, tree3, tree4, tree5)
#kernel.list = list(S_IBS, S_AM, S_AS, S_h1, skat.kernel,
#                  tree1, tree2, tree3, tree4, tree5)
names(kernel.list) = c("tree1", "tree2", "tree3",
"tree4", "tree5")
return(kernel.list)
#names(kernel.list) = c("S_IBS", "S_AM", "S_AS",
#                      "S_h1", "skat.kernel","tree1", "tree2", "tree3", "tree4", "tree5")
}
mytree = trees[[j]]
mytree_kernels = get.tree.kernels(mytree)
View(a1)
a1
a2
sim1
get.tree.kernels = function(mytree){
# Various (tree) similarity matrices
sim1=treeSimilarity(mytree) # TMRCA - time to present of pairs mrca
sim2=treeSimilarity(mytree,propn=TRUE) # (TMRCA - time to present of pairs mrca)/TMRCA
sim3=treeSimilarity(mytree, method="ranks") # Rank of coalescent event for each pairs mrca (oldest ranked 1)
sim4=treeSimilarity(mytree, method="sdscaled") # Like 1, but intercoalescence times divided by mean/sd
sim5=treeSimilarity(mytree, method="sdscaled",propn=T) # Like 2, but intercoalescence times divided by mean/sd
#Sorted issue:
tree1=order(sim1[[1]], sim1[[2]])
tree2=order(sim1[[1]], sim1[[2]])
tree3=order(sim1[[1]], sim1[[2]])
tree4=order(sim1[[1]], sim1[[2]])
tree5=order(sim1[[1]], sim1[[2]])
#Kernel Matrices
kernel.list = list(tree1, tree2, tree3, tree4, tree5)
#kernel.list = list(S_IBS, S_AM, S_AS, S_h1, skat.kernel,
#                  tree1, tree2, tree3, tree4, tree5)
names(kernel.list) = c("tree1", "tree2", "tree3",
"tree4", "tree5")
return(kernel.list)
#names(kernel.list) = c("S_IBS", "S_AM", "S_AS",
#                      "S_h1", "skat.kernel","tree1", "tree2", "tree3", "tree4", "tree5")
}
mytree_kernels = get.tree.kernels(mytree)
View(mytree_kernels[[1]])
length(status)
null.model = SKAT_Null_Model(status ~ 1, out_type="D")
SKAT.stat=vector("list", length(mykernels))
SKAT.stat=vector("list", length(mytree_kernels))
SKAT(Z=as.matrix(G), obj=null.model, kernel = mykernels[[i]])
matrix(rep(0, 516*516), nrow = 516, ncol ncol = 516)
matrix(rep(0, 516*516), nrow = 516, ncol = 516)
G=matrix(rep(0, 516*516), nrow = 516, ncol = 516)
i=1
SKAT(Z=as.matrix(G), obj=null.model, kernel = mytree_kernels[[i]])
dat=read.table("Processed_haplodata.txt")
SKAT(Z=as.matrix(dat), obj=null.model, kernel = mytree_kernels[[i]])
i=3
SKAT(Z=as.matrix(dat), obj=null.model, kernel = mytree_kernels[[i]])
i=4
SKAT(Z=as.matrix(dat), obj=null.model, kernel = mytree_kernels[[i]])
mytree_kernels[[5]]
View(mytree_kernels[[5]])
get.tree.kernels = function(mytree){
# Various (tree) similarity matrices
sim1=treeSimilarity(mytree) # TMRCA - time to present of pairs mrca
sim2=treeSimilarity(mytree,propn=TRUE) # (TMRCA - time to present of pairs mrca)/TMRCA
sim3=treeSimilarity(mytree, method="ranks") # Rank of coalescent event for each pairs mrca (oldest ranked 1)
sim4=treeSimilarity(mytree, method="sdscaled") # Like 1, but intercoalescence times divided by mean/sd
sim5=treeSimilarity(mytree, method="sdscaled",propn=T) # Like 2, but intercoalescence times divided by mean/sd
#Sorted issue:
tree1=order(sim1[[1]], sim1[[2]])
tree2=order(sim2[[1]], sim2[[2]])
tree3=order(sim3[[1]], sim3[[2]])
tree4=order(sim4[[1]], sim4[[2]])
tree5=order(sim5[[1]], sim5[[2]])
#Kernel Matrices
kernel.list = list(tree1, tree2, tree3, tree4, tree5)
#kernel.list = list(S_IBS, S_AM, S_AS, S_h1, skat.kernel,
#                  tree1, tree2, tree3, tree4, tree5)
names(kernel.list) = c("tree1", "tree2", "tree3",
"tree4", "tree5")
return(kernel.list)
#names(kernel.list) = c("S_IBS", "S_AM", "S_AS",
#                      "S_h1", "skat.kernel","tree1", "tree2", "tree3", "tree4", "tree5")
}
j
mytree = trees[[j]]
mytree_kernels = get.tree.kernels(mytree)
null.model = SKAT_Null_Model(status ~ 1, out_type="D")
SKAT.stat=vector("list", length(mytree_kernels))
G=dat
i=1
SKAT(Z=as.matrix(G), obj=null.model, kernel = mytree_kernels[[i]])
i=4
SKAT(Z=as.matrix(G), obj=null.model, kernel = mytree_kernels[[i]])
SKAT.stat[[i]]= tryCatch(SKAT(Z=as.matrix(G), obj=null.model, kernel = mytree_kernels[[i]])$Q,
error = function(e)
paste("NA"))
SKAT.stat
P1=status
phenotype1_model = lm(P1~1)
residuals_P1 = resid(phenotype1_model)
Z_ij = residuals_P1%*%t(residuals_P1)
N = sum(upper.tri(Z_ij))
n=length(status)
z1 <-c(rep (0,N))
count = 1
for (i in 1:(n -1)) {
for (j in (i +1): n) {
z1[ count ]= Z_ij[i,j]
count = count + 1
}
}
chr=c("tree1", "tree2", "tree3", "tree4", "tree5")
s_vectors = c()
kernel.list = mytree_kernels
kernel = mytree_kernels[[1]]
upper_diagonal <-c(rep (0,N))
count = 1
for (ii in 1:(n -1)) {
for (j in (ii +1): n) {
upper_diagonal[count]= kernel[ii,j]
count = count + 1
}
}
s_vectors=cbind(s_vectors,upper_diagonal)
wald.pheno1 = matrix (rep(0, length(chr)), ncol = length(chr))
s_vectors
i=1
gene_trait_lm  = lm(z1~ s_vectors[,i])
wald.pheno1[i] =(summary(gene_trait_lm)$coefficients[2 ,3])^2
wald.pheno1
as.vector(wald.pheno1)
kernel
library(MDMR)
MDMR <-mdmr(X=P1 , D = kernel, perm.p = TRUE)
MDMR
?mdmr
MDMR <-mdmr(X=P1 , D = kernel, perm.p = FALSE)
MDMR
MDMR$stat
MDMR$pv
MDMR <-mdmr(X=P1 , D = kernel, perm.p = FALSE)
MDMR$stat
MDMR <-mdmr(X=P1 , D = kernel, perm.p = TRUE)
MDMR$stat
MDMR$stat[2,1]
